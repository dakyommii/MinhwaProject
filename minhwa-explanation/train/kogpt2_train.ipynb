{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dakyommii/MinhwaProject/blob/main/minhwa-explanation/train/kogpt2_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KoGPT2"
      ],
      "metadata": {
        "id": "-YOeDwDspv2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_hhOGSZGmkE",
        "outputId": "8ddabb05-c487-417e-ab82-f0c9c4720e0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library"
      ],
      "metadata": {
        "id": "omJcVJ5rTUIq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-ZVHIOKpnKR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cbf385d-fc1d-48e9-8f87-a903e2f8b65c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (17.0.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.14.0 transformers-4.34.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "id": "HsvrYLjFprM2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77fd0874-d0ed-48c6-b067-634d7e7a0e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dataset"
      ],
      "metadata": {
        "id": "WMajy_Txp2h8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "ja5reSqRqENn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터셋 불러오기"
      ],
      "metadata": {
        "id": "cgNnPQIVXNHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# CSV 파일 경로 및 파일명 설정\n",
        "file_path = \"/content/drive/MyDrive/seol/datasets/kor_data-v4.csv\"  # 파일 경로와 파일명을 적절히 수정하세요.\n",
        "\n",
        "# 각 열을 저장할 빈 리스트 생성\n",
        "keyword_lst = []\n",
        "text_lst = []  # 필요한 만큼 열을 추가\n",
        "\n",
        "# CSV 파일 읽기\n",
        "with open(file_path, \"r\", newline=\"\") as csv_file:\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "\n",
        "    # 헤더를 건너뛰기\n",
        "    next(csv_reader)\n",
        "\n",
        "    # 각 행을 읽어와서 열별로 데이터를 리스트에 추가\n",
        "    for row in csv_reader:\n",
        "        if len(row) >= 2:  # 예를 들어, 3개의 열이 있는 경우\n",
        "            keyword_lst.append(row[0])\n",
        "            text_lst.append(row[2])"
      ],
      "metadata": {
        "id": "EiiAICygOAtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 특정 단어 포함한 텍스트를 조합하여 데이터 증강\n",
        "from itertools import combinations\n",
        "\n",
        "keywords = ['white flower', 'red flower', 'yellow flower', 'lotus', 'rock', 'bird', 'brown moran', 'white moran']\n",
        "genre='Morando'\n",
        "\n",
        "# 가능한 모든 조합 생성\n",
        "all_combinations = []\n",
        "for r in range(1, len(keywords) + 1):  # 1개부터 모든 키워드를 사용한 조합까지 생성\n",
        "    combinations_r = combinations(keywords, r)\n",
        "    all_combinations.extend(combinations_r)\n",
        "\n",
        "# 생성된 조합 출력\n",
        "combos=[]\n",
        "for combo in all_combinations:\n",
        "  s=\"\"\n",
        "  for i in range(0,len(combo)):\n",
        "    if(s!=\"\"): s+=','\n",
        "    s+=combo[i]\n",
        "\n",
        "  combos.append(s)\n",
        "\n",
        "unique_combo = list(set(combos))\n",
        "\n",
        "genre_list = [genre] * len(unique_combo)\n"
      ],
      "metadata": {
        "id": "er0FOR7CrA3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 전처리\n",
        "for i in range(0,len(text_lst)):\n",
        "  val=text_lst[i]\n",
        "  val_new = val.replace('핑크', '분홍색') # 바꿀 텍스트와 대체할 텍스트\n",
        "  text_lst[i]=val_new"
      ],
      "metadata": {
        "id": "hFH9WRCFX5-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터를 담을 딕셔너리 생성\n",
        "data = {\n",
        "    'keyword': unique_combo,\n",
        "    'explanation': exp\n",
        "}\n",
        "\n",
        "# 딕셔너리를 데이터프레임으로 변환\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 데이터프레임을 CSV 파일로 저장\n",
        "csv_file_path = \"sample_data.csv\"  # 저장할 CSV 파일의 경로 및 파일명 지정\n",
        "df.to_csv(csv_file_path, index=False)  # index=False로 설정하면 인덱스를 CSV에 저장하지 않습니다.\n"
      ],
      "metadata": {
        "id": "I-kYKdqqtbRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# CSV 파일 로드\n",
        "csv_file_path = \"sample_data.csv\"  # CSV 파일 경로를 지정해야 합니다.\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# 데이터 확인\n",
        "print(df.head())  # 데이터프레임의 처음 몇 행을 출력합니다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjDGF2pmp3rl",
        "outputId": "35cb43d3-4f0a-4a04-aa5d-a0f409b30b27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Name  Age         City\n",
            "0   John   25     New York\n",
            "1  Alice   30  Los Angeles\n",
            "2    Bob   35      Chicago\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fine-tuning"
      ],
      "metadata": {
        "id": "2RK7R2LEptEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelWithLMHead, PreTrainedTokenizerFast\n",
        "from fastai.text.all import *\n",
        "import fastai\n",
        "import re"
      ],
      "metadata": {
        "id": "7qSPXiM0F4GS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 로드"
      ],
      "metadata": {
        "id": "EVyTCfpHWpBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"/content/drive/MyDrive/seol/ckpt/kogpt2-v2\",\n",
        "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "  pad_token='<pad>', mask_token='<mask>') # 토크나이저\n",
        "model = AutoModelWithLMHead.from_pretrained(\"/content/drive/MyDrive/seol/ckpt/kogpt2-v2\")  # 모델 불러오기"
      ],
      "metadata": {
        "id": "CRLBTw7CF65i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfa2dc16-7c3e-4f75-93e2-85dd26f7383e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/modeling_auto.py:1499: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터셋 로드"
      ],
      "metadata": {
        "id": "pkIe3iVzWsmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "# CSV 파일 로드\n",
        "csv_file_path = \"/content/drive/MyDrive/seol/datasets/kor_data-v4.csv\"\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# 텍스트 데이터 추출\n",
        "keyword_col = df['keyword']\n",
        "exp_col = df['explanation']\n",
        "# 다른 열도 필요한 만큼 추가\n",
        "\n",
        "# 추출된 텍스트 데이터 리스트를 리스트로 변환\n",
        "kwd_lst = keyword_col.tolist()\n",
        "exp_lst=exp_col.tolist()\n",
        "\n",
        "# 파인튜닝에 사용할 데이터 생성\n",
        "train_texts = []\n",
        "for i in range(len(kwd_lst)):\n",
        "    combined_text = f\"{kwd_lst[i]} | {exp_lst[i]}\"  # 필요에 따라 조합\n",
        "    train_texts.append(combined_text)"
      ],
      "metadata": {
        "id": "SmcK1-3oFner"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "poUIjlt_FnmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# 데이터셋 클래스 정의\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, tokenizer, text_list, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text_list = text_list\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.text_list[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        inputs = {\n",
        "            'input_ids': encoding['input_ids'],\n",
        "            'attention_mask': encoding['attention_mask']  # attention_mask 추가\n",
        "        }\n",
        "        return inputs\n",
        "\n",
        "\n",
        "# GPU 사용 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 모델과 토크나이저 로드\n",
        "model.to(device)\n",
        "\n",
        "# 파인튜닝을 위한 데이터 준비\n",
        "train_dataset = MyDataset(tokenizer, train_texts)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# 옵티마이저 및 스케줄러 설정\n",
        "num_epochs = 200\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, num_warmup_steps=100, num_training_steps=len(train_dataloader) * num_epochs\n",
        ")\n",
        "\n",
        "# 초기 손실을 큰 값으로 설정\n",
        "best_loss = float(\"inf\")\n",
        "\n",
        "# 파인튜닝 하기\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in train_dataloader:\n",
        "      inputs = {\n",
        "          'input_ids': batch['input_ids'].to(device),\n",
        "          'attention_mask': batch['attention_mask'].to(device)\n",
        "      }\n",
        "      outputs = model(**inputs, labels=inputs['input_ids'])  # labels도 input_ids와 동일하게 설정\n",
        "      loss = outputs.loss\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "      total_loss += loss.item()\n",
        "\n",
        "    # 평균 손실 계산\n",
        "    average_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss}\")\n",
        "\n",
        "    # 현재 모델의 손실이 이전 모델보다 낮으면 모델 저장\n",
        "    if average_loss < best_loss:\n",
        "        best_loss = average_loss\n",
        "        model.save_pretrained(\"/content/drive/MyDrive/seol/ckpt/kogpt2-v2\")\n",
        "        tokenizer.save_pretrained(\"/content/drive/MyDrive/seol/ckpt/kogpt2-v2\")\n"
      ],
      "metadata": {
        "id": "XVnessWypuuy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86678015-6d13-48ff-8663-4b8752049d72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200, Loss: 0.010462106732528903\n",
            "Epoch 2/200, Loss: 0.015016637265704527\n",
            "Epoch 3/200, Loss: 0.012024304029269668\n",
            "Epoch 4/200, Loss: 0.011958024603967181\n",
            "Epoch 5/200, Loss: 0.011551800384716003\n",
            "Epoch 6/200, Loss: 0.010829576257683203\n",
            "Epoch 7/200, Loss: 0.01144684761941076\n",
            "Epoch 8/200, Loss: 0.01297617970804076\n",
            "Epoch 9/200, Loss: 0.012858183694791473\n",
            "Epoch 10/200, Loss: 0.011247409538304555\n",
            "Epoch 11/200, Loss: 0.012185397917616688\n",
            "Epoch 12/200, Loss: 0.014184534187222313\n",
            "Epoch 13/200, Loss: 0.011267567828609914\n",
            "Epoch 14/200, Loss: 0.010812439498474855\n",
            "Epoch 15/200, Loss: 0.010198725660716345\n",
            "Epoch 16/200, Loss: 0.010497601903573482\n",
            "Epoch 17/200, Loss: 0.011048075402835885\n",
            "Epoch 18/200, Loss: 0.010646992859369266\n",
            "Epoch 19/200, Loss: 0.011012727135379693\n",
            "Epoch 20/200, Loss: 0.012480446226716398\n",
            "Epoch 21/200, Loss: 0.01141697684202544\n",
            "Epoch 22/200, Loss: 0.010763363901548043\n",
            "Epoch 23/200, Loss: 0.011212481102142149\n",
            "Epoch 24/200, Loss: 0.010685946608330318\n",
            "Epoch 25/200, Loss: 0.012042481970867354\n",
            "Epoch 26/200, Loss: 0.011672581278323057\n",
            "Epoch 27/200, Loss: 0.011922909939538933\n",
            "Epoch 28/200, Loss: 0.01056510060579477\n",
            "Epoch 29/200, Loss: 0.0104374265894963\n",
            "Epoch 30/200, Loss: 0.010621339977791388\n",
            "Epoch 31/200, Loss: 0.010826826697873498\n",
            "Epoch 32/200, Loss: 0.011918727966571997\n",
            "Epoch 33/200, Loss: 0.011082625571459889\n",
            "Epoch 34/200, Loss: 0.011059575354468502\n",
            "Epoch 35/200, Loss: 0.010628578080210144\n",
            "Epoch 36/200, Loss: 0.010558615221145625\n",
            "Epoch 37/200, Loss: 0.01043544290587306\n",
            "Epoch 38/200, Loss: 0.011111242261653888\n",
            "Epoch 39/200, Loss: 0.010999088047328824\n",
            "Epoch 40/200, Loss: 0.010028672385068532\n",
            "Epoch 41/200, Loss: 0.010056861453204455\n",
            "Epoch 42/200, Loss: 0.009765855725408492\n",
            "Epoch 43/200, Loss: 0.009658841973917927\n",
            "Epoch 44/200, Loss: 0.00975004047389605\n",
            "Epoch 45/200, Loss: 0.009757427253319831\n",
            "Epoch 46/200, Loss: 0.010037310195607161\n",
            "Epoch 47/200, Loss: 0.012752143423558173\n",
            "Epoch 48/200, Loss: 0.011896236445531696\n",
            "Epoch 49/200, Loss: 0.01025047272298775\n",
            "Epoch 50/200, Loss: 0.009941638857237771\n",
            "Epoch 51/200, Loss: 0.009745228358036923\n",
            "Epoch 52/200, Loss: 0.009519550291274836\n",
            "Epoch 53/200, Loss: 0.009439208670268337\n",
            "Epoch 54/200, Loss: 0.009429298383100125\n",
            "Epoch 55/200, Loss: 0.00952069432904085\n",
            "Epoch 56/200, Loss: 0.009627250778840152\n",
            "Epoch 57/200, Loss: 0.01042785445049762\n",
            "Epoch 58/200, Loss: 0.010962507099158572\n",
            "Epoch 59/200, Loss: 0.010545812449613196\n",
            "Epoch 60/200, Loss: 0.009891139482800475\n",
            "Epoch 61/200, Loss: 0.009530240998876666\n",
            "Epoch 62/200, Loss: 0.00967024636181618\n",
            "Epoch 63/200, Loss: 0.010546552689333341\n",
            "Epoch 64/200, Loss: 0.009994238023897132\n",
            "Epoch 65/200, Loss: 0.009918385988357896\n",
            "Epoch 66/200, Loss: 0.009643289556135675\n",
            "Epoch 67/200, Loss: 0.010287057881628325\n",
            "Epoch 68/200, Loss: 0.010831763549613025\n",
            "Epoch 69/200, Loss: 0.00986017040004898\n",
            "Epoch 70/200, Loss: 0.009629104049419036\n",
            "Epoch 71/200, Loss: 0.00985594858794646\n",
            "Epoch 72/200, Loss: 0.009662499737752947\n",
            "Epoch 73/200, Loss: 0.01067358517216262\n",
            "Epoch 74/200, Loss: 0.010197049834971834\n",
            "Epoch 75/200, Loss: 0.00967689620297469\n",
            "Epoch 76/200, Loss: 0.009358569709193743\n",
            "Epoch 77/200, Loss: 0.009300558278504425\n",
            "Epoch 78/200, Loss: 0.010147201134349236\n",
            "Epoch 79/200, Loss: 0.00996515968769878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 텍스트 생성 테스트"
      ],
      "metadata": {
        "id": "urQkjYyLVYTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/ckpt/kogpt2\"  # 모델과 토크나이저를 동일한 디렉토리에 저장한 경우\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
        "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "  pad_token='<pad>', mask_token='<mask>') # 토크나이저\n",
        "\n",
        "prompt = \"tiger,red flower\" ## 입력 텍스트\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# 텍스트 생성\n",
        "output = model.generate(input_ids, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95)\n",
        "\n",
        "# 생성된 텍스트 출력\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "XArYpwJ4iZoR"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}