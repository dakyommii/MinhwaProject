{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dakyommii/MinhwaProject/blob/main/minhwa-explanation/train/kogpt2-train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KoGPT2"
      ],
      "metadata": {
        "id": "-YOeDwDspv2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_hhOGSZGmkE",
        "outputId": "8ddabb05-c487-417e-ab82-f0c9c4720e0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library"
      ],
      "metadata": {
        "id": "omJcVJ5rTUIq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5-ZVHIOKpnKR"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "id": "HsvrYLjFprM2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dataset"
      ],
      "metadata": {
        "id": "WMajy_Txp2h8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "ja5reSqRqENn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터셋 불러오기"
      ],
      "metadata": {
        "id": "cgNnPQIVXNHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# CSV 파일 경로 및 파일명 설정\n",
        "file_path = \"/content/drive/MyDrive/seol/datasets/kor_data-v4.csv\"  # 파일 경로와 파일명을 적절히 수정하세요.\n",
        "\n",
        "# 각 열을 저장할 빈 리스트 생성\n",
        "keyword_lst = []\n",
        "text_lst = []  # 필요한 만큼 열을 추가\n",
        "\n",
        "# CSV 파일 읽기\n",
        "with open(file_path, \"r\", newline=\"\") as csv_file:\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "\n",
        "    # 헤더를 건너뛰기\n",
        "    next(csv_reader)\n",
        "\n",
        "    # 각 행을 읽어와서 열별로 데이터를 리스트에 추가\n",
        "    for row in csv_reader:\n",
        "        if len(row) >= 2:  # 예를 들어, 3개의 열이 있는 경우\n",
        "            keyword_lst.append(row[0])\n",
        "            text_lst.append(row[2])"
      ],
      "metadata": {
        "id": "EiiAICygOAtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 특정 단어 포함한 텍스트를 조합하여 데이터 증강\n",
        "from itertools import combinations\n",
        "\n",
        "keywords = ['white flower', 'red flower', 'yellow flower', 'lotus', 'rock', 'bird', 'brown moran', 'white moran']\n",
        "genre='Morando'\n",
        "\n",
        "# 가능한 모든 조합 생성\n",
        "all_combinations = []\n",
        "for r in range(1, len(keywords) + 1):  # 1개부터 모든 키워드를 사용한 조합까지 생성\n",
        "    combinations_r = combinations(keywords, r)\n",
        "    all_combinations.extend(combinations_r)\n",
        "\n",
        "# 생성된 조합 출력\n",
        "combos=[]\n",
        "for combo in all_combinations:\n",
        "  s=\"\"\n",
        "  for i in range(0,len(combo)):\n",
        "    if(s!=\"\"): s+=','\n",
        "    s+=combo[i]\n",
        "\n",
        "  combos.append(s)\n",
        "\n",
        "unique_combo = list(set(combos))\n",
        "\n",
        "genre_list = [genre] * len(unique_combo)\n"
      ],
      "metadata": {
        "id": "er0FOR7CrA3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 전처리\n",
        "for i in range(0,len(text_lst)):\n",
        "  val=text_lst[i]\n",
        "  val_new = val.replace('핑크', '분홍색') # 바꿀 텍스트와 대체할 텍스트\n",
        "  text_lst[i]=val_new"
      ],
      "metadata": {
        "id": "hFH9WRCFX5-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터를 담을 딕셔너리 생성\n",
        "data = {\n",
        "    'keyword': unique_combo,\n",
        "    'explanation': exp\n",
        "}\n",
        "\n",
        "# 딕셔너리를 데이터프레임으로 변환\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 데이터프레임을 CSV 파일로 저장\n",
        "csv_file_path = \"sample_data.csv\"  # 저장할 CSV 파일의 경로 및 파일명 지정\n",
        "df.to_csv(csv_file_path, index=False)  # index=False로 설정하면 인덱스를 CSV에 저장하지 않습니다.\n"
      ],
      "metadata": {
        "id": "I-kYKdqqtbRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# CSV 파일 로드\n",
        "csv_file_path = \"sample_data.csv\"  # CSV 파일 경로를 지정해야 합니다.\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# 데이터 확인\n",
        "print(df.head())  # 데이터프레임의 처음 몇 행을 출력합니다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjDGF2pmp3rl",
        "outputId": "35cb43d3-4f0a-4a04-aa5d-a0f409b30b27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Name  Age         City\n",
            "0   John   25     New York\n",
            "1  Alice   30  Los Angeles\n",
            "2    Bob   35      Chicago\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fine-tuning"
      ],
      "metadata": {
        "id": "2RK7R2LEptEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelWithLMHead, PreTrainedTokenizerFast\n",
        "from fastai.text.all import *\n",
        "import fastai\n",
        "import re"
      ],
      "metadata": {
        "id": "7qSPXiM0F4GS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 로드"
      ],
      "metadata": {
        "id": "EVyTCfpHWpBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"/content/drive/MyDrive/seol/ckpt/kogpt2-v2\",\n",
        "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "  pad_token='<pad>', mask_token='<mask>') # 토크나이저\n",
        "model = AutoModelWithLMHead.from_pretrained(\"/content/drive/MyDrive/seol/ckpt/kogpt2-v2\")  # 모델 불러오기"
      ],
      "metadata": {
        "id": "CRLBTw7CF65i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfa2dc16-7c3e-4f75-93e2-85dd26f7383e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/modeling_auto.py:1499: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터셋 로드"
      ],
      "metadata": {
        "id": "pkIe3iVzWsmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "# CSV 파일 로드\n",
        "csv_file_path = \"/content/drive/MyDrive/seol/datasets/kor_data-v4.csv\"\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# 텍스트 데이터 추출\n",
        "keyword_col = df['keyword']\n",
        "exp_col = df['explanation']\n",
        "# 다른 열도 필요한 만큼 추가\n",
        "\n",
        "# 추출된 텍스트 데이터 리스트를 리스트로 변환\n",
        "kwd_lst = keyword_col.tolist()\n",
        "exp_lst=exp_col.tolist()\n",
        "\n",
        "# 파인튜닝에 사용할 데이터 생성\n",
        "train_texts = []\n",
        "for i in range(len(kwd_lst)):\n",
        "    combined_text = f\"{kwd_lst[i]} | {exp_lst[i]}\"  # 필요에 따라 조합\n",
        "    train_texts.append(combined_text)"
      ],
      "metadata": {
        "id": "SmcK1-3oFner"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "poUIjlt_FnmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# 데이터셋 클래스 정의\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, tokenizer, text_list, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text_list = text_list\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.text_list[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        inputs = {\n",
        "            'input_ids': encoding['input_ids'],\n",
        "            'attention_mask': encoding['attention_mask']  # attention_mask 추가\n",
        "        }\n",
        "        return inputs\n",
        "\n",
        "\n",
        "# GPU 사용 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 모델과 토크나이저 로드\n",
        "model.to(device)\n",
        "\n",
        "# 파인튜닝을 위한 데이터 준비\n",
        "train_dataset = MyDataset(tokenizer, train_texts)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# 옵티마이저 및 스케줄러 설정\n",
        "num_epochs = 200\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, num_warmup_steps=100, num_training_steps=len(train_dataloader) * num_epochs\n",
        ")\n",
        "\n",
        "# 초기 손실을 큰 값으로 설정\n",
        "best_loss = float(\"inf\")\n",
        "\n",
        "# 파인튜닝 하기\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in train_dataloader:\n",
        "      inputs = {\n",
        "          'input_ids': batch['input_ids'].to(device),\n",
        "          'attention_mask': batch['attention_mask'].to(device)\n",
        "      }\n",
        "      outputs = model(**inputs, labels=inputs['input_ids'])  # labels도 input_ids와 동일하게 설정\n",
        "      loss = outputs.loss\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "      total_loss += loss.item()\n",
        "\n",
        "    # 평균 손실 계산\n",
        "    average_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss}\")\n",
        "\n",
        "    # 현재 모델의 손실이 이전 모델보다 낮으면 모델 저장\n",
        "    if average_loss < best_loss:\n",
        "        best_loss = average_loss\n",
        "        model.save_pretrained(\"/content/drive/MyDrive/seol/ckpt/kogpt2-v2\")\n",
        "        tokenizer.save_pretrained(\"/content/drive/MyDrive/seol/ckpt/kogpt2-v2\")\n"
      ],
      "metadata": {
        "id": "XVnessWypuuy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 텍스트 생성 테스트"
      ],
      "metadata": {
        "id": "urQkjYyLVYTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/ckpt/kogpt2\"  # 모델과 토크나이저를 동일한 디렉토리에 저장한 경우\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
        "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "  pad_token='<pad>', mask_token='<mask>') # 토크나이저\n",
        "\n",
        "prompt = \"tiger,red flower\" ## 입력 텍스트\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# 텍스트 생성\n",
        "output = model.generate(input_ids, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95)\n",
        "\n",
        "# 생성된 텍스트 출력\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "XArYpwJ4iZoR"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}